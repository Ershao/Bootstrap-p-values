{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapped p values for the two-sample t test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Jupyter notebook having two purposes.\n",
    "\n",
    "1. Explain how to get bootstrap p values \n",
    "\n",
    "2. Show how blazingly fast Python is for this kind of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap is a resampling technique introduced by Bradley Efron in 1979 as a tool to find standard errors and confidence intervals in settings where we \n",
    "\n",
    "- do not have a standard error at hand for the given estimator (because it is too complex) or\n",
    "\n",
    "- think that assumptions behind the usual standard error are grossly violated.\n",
    "\n",
    "If you are not yet familiar with this technique, check <a href=\"https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\" >Wikipedia</a>. \n",
    "\n",
    "How do you e.g. calculate the Bootstrap standard error of the sample mean of $n$ observations $X_1, \\dots, X_n$?\n",
    "1. Calculate the sample mean $\\bar X$\n",
    "2. Draw with replacement $n$ observations from the sample $X_1, \\dots, X_n$. This is your first Bootstrap sample. Repeat this over and over to end up with $B$ Bootstrap samples. Choose $B$ as large as possible, e.g. 10'000. The whole point of the Bootstrap is that such Bootstrap sample is to the original sample as the sample is to the population.\n",
    "3. For each bootstrap sample, calculate the sample mean $\\bar X_i^b$. These values form your bootstrap sampling distribution.\n",
    "\n",
    "The sample standard deviation $S$ of $\\bar X_1^b, \\dots, \\bar X_B^b$ is often a reasonable guess for the true standard error of the mean. The interval $[\\bar X \\pm 1.96 S]$ is an approximate 95%-confidence interval for the true population mean $\\mu$. Alternatively, the percentage Bootstrap confidence interval can be calculated, which is just the range from the 2.5% quantile up to the 97.5% quantile of the values $\\bar X_i^b$. These are just simple Bootstrap confidence intervals. There are clearly better versions around, no question. But our focus today is on something more delicate: p values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p values via bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its simplicity, researchers are tempted also in finding p values by the bootstrap. Often a bad idea:\n",
    "- Almost always, permutation tests are the way to go if we do not trust classic tests. An exception is when the observations cannot be assumed to be exchangable under the null, e.g. in a two-sample t-test situation with unequal variances.\n",
    "\n",
    "- Often, classic tests (e.g. Welch's two-sample t test) are quite robust even to clear violations of assumptions.\n",
    "\n",
    "- They do it wrongly. \n",
    "\n",
    "What do I mean by the last bullet point? Look at the following algorithm:\n",
    "\n",
    "1. Calculate original value of the test statistic $T_0$\n",
    "\n",
    "2. Draw $B$ boostrap samples\n",
    "\n",
    "3. For each bootstrap sample, calculate the test statistic $T_i$. The values $T_1, ..., T_B$ form the (boostrap) sampling distribution used in subsequent steps.\n",
    "\n",
    "4. Calculate the proportion $\\hat p$ of values in $T_1, ..., T_B$ at least as large as $T_0$. In a two-sided setting, the p value is $2 \\cdot \\min\\{\\hat p, 1-\\hat p\\}$.\n",
    "\n",
    "My completely unqualified guess is that 80% of all bootstrap p values are calculated like this. The problem is hidden in Step 3 and we need to modify just this step: The boostrap sampling distribution is centered approximately around $T_0$ instead of 0 (or whatever value is associated with the null hypothesis). We are not interested in the sampling distribution around our specific value of the test statistic. But instead, we need to find the sampling distribution under the null hypothesis. In a two-sample t test setting, you would just subtract the group means from each value to end up with two samples with equal mean (0). Similar in a k-sample comparison setting: just subtract the location estimate of interest. An excellent reference is [Boos & Brownie (1988)](https://pdfs.semanticscholar.org/ba4e/96f388ee8fc03e78779ba1d1a303174e420c.pdf). \n",
    "\n",
    "- It explains in detail how to find p values by the Bootstrap\n",
    "- It uses awesome tricks in Monte-Carlo simulation\n",
    "- It compares different two- and k-sample tests among others.\n",
    "\n",
    "We will focus now on the comparison of two means. We can bootstrap one of the following test statistics: \n",
    "\n",
    "1. The mean difference\n",
    "2. The classic t-test statistic, i.e. the mean difference normalized\n",
    "3. <a href = \"https://en.wikipedia.org/wiki/Welch%27s_t-test\"> Welch's t test statistic</a>, which uses a different normalization than the classic t-test.\n",
    "\n",
    "In the paper cited above, the authors recommend to use the third option. It had best performance regarding type I and II errors among the other procedures in consideration.  \n",
    "\n",
    "So let's implement the Bootstrap p value for Welch's t-test statistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for calculatin Bootstrap p values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually I do statistics in R. But it is difficult to beat Python in this setting. No loop, no comprehension... increadible. I will pay you a coffee if you manage to provide a faster version in R ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the functions.\n",
    "\n",
    "1. `boot_matrix`: It creates $B$ bootstrap samples of a vector and returns the result in a matrix\n",
    "\n",
    "2. `bootstrap_t_pvalue`: It takes two samples and returns the bootstrap p value along with the original two-sample t test statistic and its parametric p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def boot_matrix(z, B):\n",
    "    \"\"\"Bootstrap sample\n",
    "    \n",
    "    Vector z is bootstrapped B times and organized in matrix\"\"\"\n",
    "    \n",
    "    n = len(z)  # sample size\n",
    "    idz = np.random.randint(0, n, (B, n))  # indices to pick for all boostrap samples\n",
    "    return z[idz]\n",
    "\n",
    "def bootstrap_t_pvalue(x, y, B=1000, equal_var=False):\n",
    "    \"\"\" Bootstrap p values for two-sample t test\n",
    "    \n",
    "    Returns tuple with boostrapped p value, test statistics and parametric p value\"\"\"\n",
    "    \n",
    "    # Original Welch's t test statistic\n",
    "    orig = stats.ttest_ind(x, y, equal_var=equal_var)\n",
    "    \n",
    "    # Generate boostrap distribution of Welch's t statistic\n",
    "    xboot = boot_matrix(x - x.mean(), B=B) # important centring step to get sampling distribution under the null\n",
    "    yboot = boot_matrix(y - y.mean(), B=B)\n",
    "    t = stats.ttest_ind(xboot, yboot, axis=1, equal_var=equal_var)[0]\n",
    "\n",
    "    # Calculate proportion of bootstrap samples with at least as strong evidence against null    \n",
    "    p = np.mean(t >= orig[0])\n",
    "    \n",
    "    return (2*min(p, 1-p), *orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first have a look at a standard situation: We sample from two shifted normal distributions. In such setting, the classic t-test is definitively the test to apply, especially because we have equal variances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.21300000000000008, -1.2299392326284944, 0.22753807122611611)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(984564) # for reproducability\n",
    "x = np.random.normal(loc=11, scale=20, size=30)\n",
    "y = np.random.normal(loc=15, scale=20, size=20)\n",
    "%time bootstrap_t_pvalue(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Our boostrap was super fast (15 milliseconds for 10'000 boostrap runs? C'mon) and returned almost the same p value as Welch's t test for unequal variances (default t test in R software). Compare first value with last value in the tuple. \n",
    "\n",
    "What would we get in such situation with the wrong approach done so frequently? We have to modify one of our two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def the_wrong_way(x, y, B=1000, equal_var=False):\n",
    "    \"\"\" Bootstrap p values for two-sample t test\n",
    "    \n",
    "    Returns tuple with boostrapped p value, test statistics and parametric p value\"\"\"\n",
    "    \n",
    "    # Original Welch's t test statistic\n",
    "    orig = stats.ttest_ind(x, y, equal_var=equal_var)\n",
    "    \n",
    "    # Generate boostrap distribution of Welch's t statistic\n",
    "    xboot = boot_matrix(x, B=B) # error\n",
    "    yboot = boot_matrix(y, B=B) # error\n",
    "    t = stats.ttest_ind(xboot, yboot, axis=1, equal_var=equal_var)[0]\n",
    "\n",
    "    # Calculate proportion of bootstrap samples with at least as strong evidence against null    \n",
    "    p = np.mean(t >= orig[0])\n",
    "    \n",
    "    return (2*min(p, 1-p), *orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98040000000000005, -1.2299392326284944, 0.22753807122611611)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_wrong_way(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just look at this p value. So wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just to show how well Welch's t-test for unequal variances works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.215, 1.2929322360150164, 0.20279745745876884)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(345244) # for reproducability\n",
    "x = np.random.normal(loc=11, scale=20, size=30)\n",
    "y = np.random.normal(loc=15, scale=10, size=20)\n",
    "bootstrap_t_pvalue(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the nasty situation: Unequal variances, non-normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.028799999999999999, 2.0977748141544903, 0.041263608229049308)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(399888) # for reproducability\n",
    "x = np.random.exponential(scale=20, size=30)\n",
    "y = np.random.exponential(scale=10, size=20)\n",
    "bootstrap_t_pvalue(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we can see quite some difference. We don't know the \"correct\" p value. To do so, we would need to do a full fledged Monte-Carlo-Study. Maybe I will do that in the next notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
