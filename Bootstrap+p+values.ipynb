{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapped p values for the two-sample t test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a short Python notebook having two purposes.\n",
    "\n",
    "1. Explain how to get bootstrap p values \n",
    "\n",
    "2. Show how blazingly fast Python is for this kind of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap is a resampling technique introduced by Bradley Efron 1979 mainly as a tool to find standard errors and confidence intervals in complex settings where we \n",
    "\n",
    "- do not have a standard error at hand for the given estimator (because it is too complex) or\n",
    "\n",
    "- know that assumptions behind the usual standard error are grossly violated.\n",
    "\n",
    "If you are not yet familiar with this technique, check [Wikipedia](https://en.wikipedia.org/wiki/Bootstrapping_(statistics). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p values via bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers are often interested also in finding p values by the bootstrap. Often a bad idea:\n",
    "\n",
    "- Almost always, permutatation tests are the way to go if we do not trust classic tests. An exception is when the observations cannot be assumed to be exchangable under the null, e.g. in a two-sample t-test situation with unequal variances.\n",
    "\n",
    "- Often, classic tests (e.g. Welch's two-sample t test) are quite robust even to clear violations of assumptions.\n",
    "\n",
    "- They do it wrongly. \n",
    "\n",
    "What do I mean by the last bullet point? Look at the following algorithm:\n",
    "\n",
    "1. Calculate original test statistic $T$\n",
    "\n",
    "2. Take $B$ boostrap samples\n",
    "\n",
    "3. For each boostrap sample, calculate the test statistic. The $B$ values form the (boostrap) sampling distribution.\n",
    "\n",
    "4. Calculate the proportion $\\hat p$ of values in the sampling distribution at least as large as $T$. In a two-sided setting (we are focussing on that one), the p value is $2 \\cdot \\{\\min{\\hat p, 1-\\hat p}\\}$.\n",
    "\n",
    "My completely random guess is that 80% of all bootstrap p values are calculated like this. The problem is hidden in Step 3: The boostrap sampling distribution is centered approximately around $T$ instead of 0 (or whatever value is associated with the null hypothesis). We are not interested in the sampling distribution around our specific value of the test statistic. But instead, we need to find the sampling distribution under the null hypothesis. This is not always easy to do. But in a two-sample t test setting, you would just subtract the group means from each value to end up with two samples with equal mean (0). Similar in a k-sample comparison setting, just subtract the location estimate of interest. An excellent reference is [Boos & Brownie (1988)](https://pdfs.semanticscholar.org/ba4e/96f388ee8fc03e78779ba1d1a303174e420c.pdf). \n",
    "\n",
    "- It explains in detail how to find p values by Boostrap\n",
    "- It contains some awesome tricks for Monte-Carlo-Comparison of statistical hypothesis tests\n",
    "- It compares different two- and k-sample tests.\n",
    "\n",
    "We will focus now on the comparison of two means. There are different possible test statistics to boostrap: \n",
    "\n",
    "1. The mean difference\n",
    "2. The studentized mean difference\n",
    "3. Welch's t test statistic\n",
    "\n",
    "In the paper cited above, based on simulations, the authors recommend to use the third option. It had best performance regarding level I and II errors among the other procedures.\n",
    "\n",
    "Now we are going to implement the Boostrap p value for Welch's t-test statistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for calculatin Bootstrap p values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually I do statistics in R. But it is difficult to beat Python in this setting. No loop, no comprehension... increadible. It will be difficult to beat the following code in R regarding execution speed. I will pay you a coffee if you manage to do so ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the functions.\n",
    "\n",
    "1. `boot_matrix`: It creates $B$ bootstrap samples of a vector and returns the result in a matrix\n",
    "\n",
    "2. `bootstrap_t_pvalue`: It takes two samples and returns the bootstrap p value along with the original two-sample t test statistic and its parametric p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def boot_matrix(z, B):\n",
    "    \"\"\"Bootstrap sample\n",
    "    \n",
    "    Vector z is bootstrapped B times and organized in matrix\"\"\"\n",
    "    \n",
    "    n = len(z)  # sample size\n",
    "    idz = np.random.randint(0, n, (B, n))  # indices to pick for all boostrap samples\n",
    "    return z[idz]\n",
    "\n",
    "def bootstrap_t_pvalue(x, y, B=1000, equal_var=False):\n",
    "    \"\"\" Bootstrap p values for two-sample t test\n",
    "    \n",
    "    Returns tuple with boostrapped p value, test statistics and parametric p value\"\"\"\n",
    "    \n",
    "    # Original Welch's t test statistic\n",
    "    orig = stats.ttest_ind(x, y, equal_var=equal_var)\n",
    "    \n",
    "    # Generate boostrap distribution of Welch's t statistic\n",
    "    xboot = boot_matrix(x - x.mean(), B=B)\n",
    "    yboot = boot_matrix(y - y.mean(), B=B)\n",
    "    t = stats.ttest_ind(xboot, yboot, axis=1, equal_var=equal_var)[0]\n",
    "\n",
    "    # Calculate proportion of bootstrap samples with at least as strong evidence against null    \n",
    "    p = np.mean(t >= orig[0])\n",
    "    \n",
    "    return (2*min(p, 1-p), *orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first have a look at a standard situation: We sample from two shifted normal distributions. In such setting, the classic t-test is definitively the test to apply, especially because we have equal variances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6956, -0.38840021321462054, 0.69944437801476789)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(loc=11, scale=20, size=30)\n",
    "y = np.random.normal(loc=15, scale=20, size=20)\n",
    "%time bootstrap_t_pvalue(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Our boostrap was super fast (15 micro seconds for 10'000 boostrap runs? Cmon) and returned almost the same p value as Welch's t test for unequal variances (default t test in R software). Compare first value with last value in the tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just to show how well Welch's t-test for unequal variances works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2914000000000001, -1.0449868988384667, 0.30210339327696162)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(loc=11, scale=20, size=30)\n",
    "y = np.random.normal(loc=15, scale=10, size=20)\n",
    "bootstrap_t_pvalue(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the nasty situation: Unequal variances, non-normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.028799999999999999, 2.0977748141544903, 0.041263608229049308)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(399888)\n",
    "x = np.random.exponential(scale=20, size=30)\n",
    "y = np.random.exponential(scale=10, size=20)\n",
    "bootstrap_t_pvalue(x, y, B=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we can see quite some difference. We don't know the \"correct\" p value. To do so, we would need to do a full fledged Monte-Carlo-Study. Maybe I will do that in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
